---
layout: post
title: "머신러닝 개념에 발담구기"
subtitle: 1차 정리
date: 2020-06-17 1:32:28 -0400
cover-img: /assets/img/ml.jpeg
tags: [machine_learning, perceptron, adaline]
---

# 퍼셉트론이란?

![image](https://user-images.githubusercontent.com/37768791/84805794-c0112a00-b03f-11ea-9198-3a13d9e8592e.png)

위 사진을 보면 y로 2개의 노드가 연결되어 있는 걸 볼 수 있습니다. 여기서 y노드를 퍼셉트론이라고 합니다. 결국 그림과 같이 다수의 신호를 받아서 하나의 신호로 출력하는 것입니다.

## 논리 게이트들

- AND 게이트
- NAND 게이트
- OR 게이트
- XOR 게이트

<br>

# 아달린이란?

ADALINE = `ADA`ptive `LI`near `NE`uron의 줄임말로 적응형 선형 뉴런을 의미합니다.

간단히 얘기하면 퍼셉트론의 향상된 버전입니다.

그럼 퍼셉트론과 어떤 차이점이 있을까요?

- 비용함수
- 선형 활성화 함수

<center>
<img src="https://user-images.githubusercontent.com/37768791/84806771-2cd8f400-b041-11ea-8443-7f90f1d247a9.png">
</center>

그림에서 볼 수 있듯이 시그마 아이콘이 있는 최종 입력 함수 다음에 선형 활성화 함수(activation function)가 추가된 것을 볼 수 있습니다.

그래서 아달린 알고리즘은 `진짜 클래스 레이블(관측값)`과 `선형 활성화 함수의 실수 출력 값`을 비교합니다.

<br>

### 경사하강법은 무엇인가?

<br>

왜 아달린에 `경사하강법`이 나왔을까요?

경사하강법은 비용함수(에러)를 `최소화`하기 위한 방법 중 하나입니다. 아까 설명했듯이 아달린에서 비용함수를 사용하기 때문이죠.

`[원리]`

SSE(잔차제곱합)으로 가중치 학습에 사용될 비용함수를 정의합니다.

비용함수 = sum(관측값 - 선형 활성화 함수 출력값) / 2

일단 비용함수는 구했는데 그럼 최소화는 어떻게 시킬까요?

<center>
<img src="https://user-images.githubusercontent.com/37768791/84807485-47f83380-b042-11ea-8d62-a904d60b36d8.png">
</center>

새로 가중치를 업데이트해서 점점 가중치를 줄이는 과정인데 그럼 가중치 업데이트는?

새로운 가중치 = -학습률 \* 비용함수

- 학습률은 어떻게 정해지는가?

  에포크 수를 조정해서 그래프를 그려가면서 최소화하는 구간으로 가는지 시각적으로 확인해가면서 정합니다.

<br>

### 특성 스케일 조정을 통한 경사하강법 성능 최적화

특성 스케일을 조정하면 경사하강법의 성능 최적화를 이룰 수 있습니다.

특성 스케일 조정 방법 중 `표준화`를 사용해보겠습니다.

`표준화 방법`

- 데이터에 표준 정규 분포 성질 부여(더 적은 단계를 거쳐 학습의 속도 높이고 최적의 솔루션을 찾을 수 있음)

<br>

### 확률적 경사하강법(stochastic gradient descent)

: 매우 큰 데이터셋인 경우 기존의 경사하강법은 계산 비용이 너무 많이 소모됩니다. 매번 `전체 훈련 데이터셋`에 누적된 오차의 합을 기반으로 가중치를 업데이트해야 하기 때문이죠. 그러나 확률적 경사하강법은 `각 훈련 샘플`에 대해서 조금씩 가중치를 업데이트합니다.

`[특징]`

1. 샘플 순서 무작위
2. 새로운 훈련 데이터가 도차하는대로 훈련(무작위이기 때문)
3. 가중치를 다시 초기화하지 않음.
4. 스트리밍 데이터에 주로 사용

<br>

# 분류 모델

- 분류의 과정

```
샘플 모으기 -> 성능 지표 설정(컬럼 설정 등등) -> 분류 알고리즘 선택 -> 모델 성능 평가(잘 분류하는가) -> 알고리즘 튜닝(결과를 잘 산출하게)
```

<br>

### 로지스틱 회귀(Logistic Regression) <span style="color:red">중요</span>

<center>
<img src="https://user-images.githubusercontent.com/37768791/84855453-1ceffd00-b09f-11ea-8cda-e80cc4be503a.png">
</center>

퍼셉트론의 한계

- 클래스를 그렸을 때 선형적으로 구분되지 않을 때 수렴 불가능

위 문제점을 해결할 수 있는 것이 바로 `로지스틱 회귀`입니다.

로지스틱 회귀도 2개의 클래스를 분류하기 위한 선형모델이지만 다중 분류도 확장할 수 있기 때문에 퍼셉트론의 한계를 극복할 수 있습니다.

`[특징]`

- 오즈비(특정 이벤트가 발생할 확률) -> P / (1-P) (P는 예측하려는 대상일 확률)
- 로짓 함수(오즈비에 자연로그 씌움) -> logit(P) = log(P / (1-P))
- 로지스틱 시그모이드 함수(로지스틱 회귀에서의 활성화 함수)
