---
layout: post
title: "머신러닝 개념에 발담구기"
subtitle: 1차 정리
date: 2020-06-17 1:32:28 -0400
cover-img: /assets/img/ml.jpeg
tags: [machine_learning, perceptron, adaline]
use_math: true
---

# 퍼셉트론이란?

![image](https://user-images.githubusercontent.com/37768791/84805794-c0112a00-b03f-11ea-9198-3a13d9e8592e.png)

위 사진을 보면 y로 2개의 노드가 연결되어 있는 걸 볼 수 있습니다. 여기서 y노드를 퍼셉트론이라고 합니다. 결국 그림과 같이 다수의 신호를 받아서 하나의 신호로 출력하는 것입니다.

## 논리 게이트들

- AND 게이트
- NAND 게이트
- OR 게이트
- XOR 게이트

<br>

# 아달린이란?

ADALINE = `ADA`ptive `LI`near `NE`uron의 줄임말로 적응형 선형 뉴런을 의미합니다.

간단히 얘기하면 퍼셉트론의 향상된 버전입니다.

그럼 퍼셉트론과 어떤 차이점이 있을까요?

- 비용함수 = J(w)
- 선형 활성화 함수

<center>
<img src="https://user-images.githubusercontent.com/37768791/84806771-2cd8f400-b041-11ea-8443-7f90f1d247a9.png">
</center>

그림에서 볼 수 있듯이 시그마 아이콘이 있는 최종 입력 함수 다음에 선형 활성화 함수(activation function)가 추가된 것을 볼 수 있습니다.

그래서 아달린 알고리즘은 `진짜 클래스 레이블(관측값)`과 `선형 활성화 함수의 실수 출력 값`을 비교합니다.

<br>

### 경사하강법은 무엇인가?

<br>

왜 아달린에 `경사하강법`이 나왔을까요?

경사하강법은 비용함수(에러)를 `최소화`하기 위한 방법 중 하나입니다. 아까 설명했듯이 아달린에서 비용함수를 사용하기 때문이죠.

`[원리]`

SSE(잔차제곱합)으로 가중치 학습에 사용될 비용함수를 정의합니다.

비용함수 = sum(관측값 - 선형 활성화 함수 출력값) / 2

일단 비용함수는 구했는데 그럼 최소화는 어떻게 시킬까요?

<center>
<img src="https://user-images.githubusercontent.com/37768791/84807485-47f83380-b042-11ea-8d62-a904d60b36d8.png">
</center>

새로 가중치를 업데이트해서 점점 가중치를 줄이는 과정인데 그럼 가중치 업데이트는?

새로운 가중치 = -학습률 \* 비용함수

- 학습률은 어떻게 정해지는가?

  에포크 수를 조정해서 그래프를 그려가면서 최소화하는 구간으로 가는지 시각적으로 확인해가면서 정합니다.

<br>

### 특성 스케일 조정을 통한 경사하강법 성능 최적화

특성 스케일을 조정하면 경사하강법의 성능 최적화를 이룰 수 있습니다.

특성 스케일 조정 방법 중 `표준화`를 사용해보겠습니다.

`표준화 방법`

- 데이터에 표준 정규 분포 성질 부여(더 적은 단계를 거쳐 학습의 속도 높이고 최적의 솔루션을 찾을 수 있음)

<br>

### 확률적 경사하강법(stochastic gradient descent)

: 매우 큰 데이터셋인 경우 기존의 경사하강법은 계산 비용이 너무 많이 소모됩니다. 매번 `전체 훈련 데이터셋`에 누적된 오차의 합을 기반으로 가중치를 업데이트해야 하기 때문이죠. 그러나 확률적 경사하강법은 `각 훈련 샘플`에 대해서 조금씩 가중치를 업데이트합니다.

`[특징]`

1. 샘플 순서 무작위
2. 새로운 훈련 데이터가 도차하는대로 훈련(무작위이기 때문)
3. 가중치를 다시 초기화하지 않음.
4. 스트리밍 데이터에 주로 사용

<br>

# 분류 모델

<br>

- 분류의 과정

```
샘플 모으기 -> 성능 지표 설정(컬럼 설정 등등) -> 분류 알고리즘 선택 -> 모델 성능 평가(잘 분류하는가) -> 알고리즘 튜닝(결과를 잘 산출하게)
```

<br>

### 나이브 베이즈(NB classification)

#### 조건부 확률

- Pr(Y\|X) = Pr(X, Y) / Pr(X)
- Pr(X, Y) = Pr(Y\|X) \* Pr(X)

#### 조건부 독립

- Pr(X1, X2 \| Y) = Pr(X1 \| Y) \* Pr(X2 \| Y)
- 확률간 곱셈이 성립하면 독립적인 관계

`[용도]`

Pr(Y\|X)만 아는 상태에서 Pr(X\|Y)를 알고자 하는 경우.

조건부 확률을 참고했을 때,  
Pr(Y\|X) = Pr(X, Y) / Pr(X) = Pr(X\|Y) \* Pr(Y) / Pr(X)

`[문제]`  
Y(HIV)감염 여부와 세 검사방법(X1, X2, X3)에 따른 양성 판정 여부(+, -) 자료이다. 어떤 사람에 대한 세 검사방법의 결과가 (+, +, +)와 같을 때, 이 사람이 HIV에 감염되었을 확률을 나이브 베이즈 분류 방법을 이용해서 구해보아라.

<center>
<img src="https://user-images.githubusercontent.com/37768791/84860021-3e55e680-b0a9-11ea-8fa7-4ad51b3276db.png">
</center>

`[풀이]`

- HIV : Y = 1
- Normal : Y = 0
- 양성 : x = 1
- 음성 : x = 0

**구하고자 하는 것**  
 => Pr(Y = 1 \| X1 = 1, X2 = 1, X3 = 1) / Pr(Y = 0 \| X1 = 1, X2 = 1, X3 = 1)

나이브 베이즈 적용
=> Pr(X1 = 1, X2 = 1, X3 = 1 \| Y=1) / Pr(X1 = 1, X2 = 1, X3 = 1 \| Y=1)

$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$

<br>

### 로지스틱 회귀(Logistic Regression) <span style="color:red">중요</span>

<center>
<img src="https://user-images.githubusercontent.com/37768791/84855453-1ceffd00-b09f-11ea-8cda-e80cc4be503a.png">
</center>

퍼셉트론의 한계

- 클래스를 그렸을 때 선형적으로 구분되지 않을 때 수렴 불가능

위 문제점을 해결할 수 있는 것이 바로 `로지스틱 회귀`입니다.

로지스틱 회귀도 2개의 클래스를 분류하기 위한 선형모델이지만 다중 분류도 확장할 수 있기 때문에 퍼셉트론의 한계를 극복할 수 있습니다.

`[특징]`

- 오즈비(특정 이벤트가 발생할 확률) -> P / (1-P) (P는 예측하려는 대상일 확률)
- 로짓 함수(오즈비에 자연로그 씌움) -> logit(P) = log(P / (1-P))
- 로지스틱 시그모이드 함수(로지스틱 회귀에서의 활성화 함수)
- 시그모이드 함수를 돌리기 때문이기도 하고 확률로 표현해야하므로 값이 0과 1사이의 값이 나옴.

`[가중치 학습 방법]`

비용함수 = sum(시그모이드 활성화 함수 출력 레이블 - 관측 레이블)^2 / 2

<center>
<img src="https://user-images.githubusercontent.com/37768791/84857180-69d5d280-b0a3-11ea-873c-1ac14b5b58e7.png">
</center>

h(x) : 시그모이드 활성화 함수에 돌렸을 때 산출된 값입니다.  
cost : 비용함수

`[비용함수 그래프 설명]`

- y = 0  
  빨간 그래프를 보면 y를 0으로 예측했을 때의 비용함수 그래프입니다. h(x)값이 0으로 예측했을 때 비용함수가 0에 수렴하는 것을 볼 수 있습니다. 잘 예측했다는 것이죠. 그러나 h(x)가 1로 향해 갈수록 비용함수가 가파르게 상승하는 것을 볼 수 있습니다. 이렇게 비용함수의 상황을 통해 분류하는 것이 로지스틱 회귀의 방법입니다.

<br>

#### 규제를 통해 과대적합 피하기

`규제`는 뭐고 `과대적합`은 무엇일까요?

- 과대적합(overfitting) : 너무 핏하게 짜여진 것을 의미하고, 분산(예측의 일관성, 민감성)이 크다고 합니다. 핏하면 좋은 것이 아닌가 하겠지만 새로운 데이터로 예측을 했을 때 잘 안맞을 확률이 있습니다.

- 과소적합(underfitting) : 모델이 충분히 복잡하지 않는 것입니다. 너무 느슨하게 짜여진 것을 의미합니다. 편향(예측에서 벗어난 정도)이 크다고도 합니다.

 <center>
 <img src="https://user-images.githubusercontent.com/37768791/84857899-08aefe80-b0a5-11ea-9cae-98bbc2931130.png">
 </center>
<center>결국 모델도 그 중간점을 잘 찾아야합니다.</center>

그러기 위해선 `규제` 가 필요합니다

추가적인 정보(편향)를 주입해서 과대적합을 방지하는 것이 `규제`입니다.

`[기존 모델에 규제 추가]`

- 규제 항인 람다 변수를 추가해줍니다.
- 람다가 증가하면 규제 강도가 높아져서 과대적합을 줄여줍니다.
- 매개변수 C : 역규제 파라미터(규제와 역수 관계)

<center>
<img src="https://user-images.githubusercontent.com/37768791/84858709-a48d3a00-b0a6-11ea-8307-782dc873f1b7.png">
</center>

위 그래프를 보면 x축에 역규제 파라미터가 있고 y축에는 가중치의 분산(민감성)이 있습니다.

좌측으로 갈수록 규제가 증가하고 우측으로 갈수록 규제가 감소합니다. 규제가 강할수록 민감도는 0에 수렴하는 것을 볼 수 있습니다.
